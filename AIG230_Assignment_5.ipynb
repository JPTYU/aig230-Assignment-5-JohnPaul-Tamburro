{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a635a0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /opt/anaconda3/lib/python3.13/site-packages (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/anaconda3/lib/python3.13/site-packages (from gensim) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /opt/anaconda3/lib/python3.13/site-packages (from gensim) (1.15.3)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in /opt/anaconda3/lib/python3.13/site-packages (from gensim) (7.5.0)\n",
      "Requirement already satisfied: wrapt in /opt/anaconda3/lib/python3.13/site-packages (from smart_open>=1.8.1->gensim) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee1d5c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a2190e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/johnpaultamburro/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/johnpaultamburro/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/johnpaultamburro/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/johnpaultamburro/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords') \n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6143260",
   "metadata": {},
   "source": [
    "Use this NLTK corpus:\n",
    "Option A (recommended): Gutenberg - 'austen-emma.txt' or 'carroll-alice.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5014db32",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = nltk.corpus.gutenberg.raw('austen-emma.txt')\n",
    "#used to read and gain a better understanding of the corpus.\n",
    "#print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ccf34d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 887071\n",
      "Number of sentences: 7752\n",
      "Total number of tokens BEFORE preprocessing: 191855\n"
     ]
    }
   ],
   "source": [
    "#A1\n",
    "raw_text = nltk.corpus.gutenberg.raw('austen-emma.txt')\n",
    "sentences = nltk.corpus.gutenberg.sents('austen-emma.txt')\n",
    "num_characters = len(raw_text)\n",
    "num_sentences = len(sentences)\n",
    "print(f\"Number of characters: {num_characters}\")\n",
    "print(f\"Number of sentences: {num_sentences}\")\n",
    "print(f\"Total number of tokens BEFORE preprocessing: {len(nltk.word_tokenize(raw_text))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28ec3611",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m vocabulary \u001b[38;5;241m=\u001b[39m preprocess(raw_text)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# preprocess each sentence separately to produce a list of token lists\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m sent_vocab \u001b[38;5;241m=\u001b[39m [preprocess(sent) \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences]\n\u001b[1;32m     20\u001b[0m vocabulary_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(vocabulary))\n\u001b[1;32m     21\u001b[0m sent_vocabulary_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m([token \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sent_vocab \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m sentence]))\n",
      "Cell \u001b[0;32mIn[19], line 10\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# treat as an iterable of tokens (e.g., list or sentence from corpus)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(token)\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m text]\n\u001b[0;32m---> 10\u001b[0m stop \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mcorpus\u001b[38;5;241m.\u001b[39mstopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop]\n\u001b[1;32m     12\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m token\u001b[38;5;241m.\u001b[39misalpha()]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/nltk/corpus/reader/wordlist.py:21\u001b[0m, in \u001b[0;36mWordListCorpusReader.words\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwords\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ignore_lines_startswith\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m     20\u001b[0m         line\n\u001b[0;32m---> 21\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m line_tokenize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw(fileids))\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstartswith(ignore_lines_startswith)\n\u001b[1;32m     23\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/nltk/corpus/reader/api.py:218\u001b[0m, in \u001b[0;36mCorpusReader.raw\u001b[0;34m(self, fileids)\u001b[0m\n\u001b[1;32m    216\u001b[0m contents \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fileids:\n\u001b[0;32m--> 218\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen(f) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[1;32m    219\u001b[0m         contents\u001b[38;5;241m.\u001b[39mappend(fp\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concat(contents)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/nltk/corpus/reader/api.py:231\u001b[0m, in \u001b[0;36mCorpusReader.open\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;124;03mReturn an open stream that can be used to read the given file.\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;124;03mIf the file's encoding is not None, then the stream will\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m:param file: The file identifier of the file to read.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    230\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding(file)\n\u001b[0;32m--> 231\u001b[0m stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_root\u001b[38;5;241m.\u001b[39mjoin(file)\u001b[38;5;241m.\u001b[39mopen(encoding)\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/nltk/data.py:325\u001b[0m, in \u001b[0;36mFileSystemPathPointer.open\u001b[0;34m(self, encoding)\u001b[0m\n\u001b[1;32m    323\u001b[0m stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 325\u001b[0m     stream \u001b[38;5;241m=\u001b[39m SeekableUnicodeStreamReader(stream, encoding)\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/nltk/data.py:1073\u001b[0m, in \u001b[0;36mSeekableUnicodeStreamReader.__init__\u001b[0;34m(self, stream, encoding, errors)\u001b[0m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;124;03mA stream reader that automatically encodes the source byte stream\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;124;03minto unicode (like ``codecs.StreamReader``); but still supports the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;124;03municode encodings.\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m DEBUG \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# : If true, then perform extra sanity checks.\u001b[39;00m\n\u001b[0;32m-> 1073\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, stream, encoding, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;66;03m# Rewind the stream to its beginning.\u001b[39;00m\n\u001b[1;32m   1075\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m stream\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#A2\n",
    "def preprocess(text):\n",
    "    # accept either raw string or a token list (e.g., a single sentence)\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "    else:\n",
    "        # treat as an iterable of tokens (e.g., list or sentence from corpus)\n",
    "        tokens = [str(token).lower() for token in text]\n",
    "    stop = nltk.corpus.stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stop]\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "vocabulary = preprocess(raw_text)\n",
    "# preprocess each sentence separately to produce a list of token lists\n",
    "sent_vocab = [preprocess(sent) for sent in sentences]\n",
    "vocabulary_size = len(set(vocabulary))\n",
    "sent_vocabulary_size = len(set([token for sentence in sent_vocab for token in sentence]))\n",
    "freqtoken = nltk.FreqDist(vocabulary)\n",
    "print(f\"Total number of characters (raw text): {len(raw_text)}\")\n",
    "print(f\"Total number of tokens AFTER preprocessing: {len(vocabulary)}\")\n",
    "#This not changing makes sense, as we are working in a character based model.\n",
    "print(f\"Total number of tokens in sentences AFTER preprocessing: {len(sent_vocab)}\")\n",
    "print(f\"Vocabulary size (unique tokens): {vocabulary_size}\")\n",
    "print(f\"Top 20 most frequent tokens: {freqtoken.most_common(20)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9935a89",
   "metadata": {},
   "source": [
    "A3\n",
    "\n",
    "My preprocessing choices will influence various tasks done in other parts. The mandatory preprocessing has already reduced the number of unqiue tokens present, reducing dimensionality and thus preventing overfitting. The initial plan was to keep stop words due to the story-like nature of the corpus, as these stop words contribute to the flow and gramatical refinement of the sentences that make up the story. However, initial running proved this to logic to be impractical and thus stop-words were removed. Using lemmatization instead of stemming will make processes slower, although the end results will be more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85136250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "#B1\n",
    "chapters = nltk.corpus.gutenberg.fileids()\n",
    "documents = []\n",
    "document_labels = []\n",
    "\n",
    "for chapter_file_id in chapters:\n",
    "    book_raw_text = nltk.corpus.gutenberg.raw(chapter_file_id)\n",
    "    chapter_strings = book_raw_text.split('CHAPTER')[1:]\n",
    "\n",
    "\n",
    "    for i, chapter_text in enumerate(chapter_strings):\n",
    "        documents.append(chapter_text)\n",
    "        document_labels.append(f\"{chapter_file_id.replace('.txt', '')} - Chapter {i+1}\")\n",
    "\n",
    "processed_documents = [' '.join(preprocess(doc)) for doc in documents]\n",
    "\n",
    "print(chapters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6ee201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n",
      "Shape of Bag-of-Words matrix: (292, 19422)\n",
      "Shape of TF-IDF matrix: (292, 19422)\n",
      "Top 15 TF-IDF terms for first document: ['taylor' 'emma' 'miss' 'weston' 'knightley' 'papa' 'match' 'woodhouse'\n",
      " 'father' 'success' 'james' 'every' 'year' 'always' 'must']\n",
      "Top 15 TF-IDF terms for second document: ['churchill' 'weston' 'miss' 'highbury' 'taylor' 'woodhouse' 'letter'\n",
      " 'frank' 'perry' 'marriage' 'never' 'randalls' 'brother' 'could' 'enough']\n"
     ]
    }
   ],
   "source": [
    "#B2\n",
    "print(chapters)\n",
    "BagofWords = CountVectorizer()\n",
    "TFIDF = TfidfVectorizer()\n",
    "BagofWords_matrix = BagofWords.fit_transform(processed_documents)\n",
    "TFIDF_matrix = TFIDF.fit_transform(processed_documents)\n",
    "print(f\"Shape of Bag-of-Words matrix: {BagofWords_matrix.shape}\")\n",
    "print(f\"Shape of TF-IDF matrix: {TFIDF_matrix.shape}\")\n",
    "\n",
    "blake15 = TFIDF_matrix[0].toarray()[0].argsort()[-15:][::-1]\n",
    "blake15_string = TFIDF.get_feature_names_out()[blake15]\n",
    "bryant15 = TFIDF_matrix[1].toarray()[0].argsort()[-15:][::-1]\n",
    "bryant15_string = TFIDF.get_feature_names_out()[bryant15]\n",
    "print(f\"Top 15 TF-IDF terms for first document:\",blake15_string)\n",
    "print(f\"Top 15 TF-IDF terms for second document:\",bryant15_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1553c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar pair of documents: carroll-alice - Chapter 9 and carroll-alice - Chapter 10 with a similarity of 75 percent.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xx/5z7hkp1d6_j8ybcbrsz0ktq40000gn/T/ipykernel_50364/586921666.py:10: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  stringed_sim_table = sim_table.applymap(lambda x: f\"{int(round(x, 2) * 100)}%\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity table:\n",
      "                        austen-emma - Chapter 1 austen-emma - Chapter 2  \\\n",
      "austen-emma - Chapter 1                      0%                     44%   \n",
      "austen-emma - Chapter 2                     44%                      0%   \n",
      "austen-emma - Chapter 3                     35%                     30%   \n",
      "austen-emma - Chapter 4                     36%                     27%   \n",
      "austen-emma - Chapter 5                     48%                     27%   \n",
      "\n",
      "                        austen-emma - Chapter 3 austen-emma - Chapter 4  \\\n",
      "austen-emma - Chapter 1                     35%                     36%   \n",
      "austen-emma - Chapter 2                     30%                     27%   \n",
      "austen-emma - Chapter 3                      0%                     33%   \n",
      "austen-emma - Chapter 4                     33%                      0%   \n",
      "austen-emma - Chapter 5                     33%                     42%   \n",
      "\n",
      "                        austen-emma - Chapter 5  \n",
      "austen-emma - Chapter 1                     48%  \n",
      "austen-emma - Chapter 2                     27%  \n",
      "austen-emma - Chapter 3                     33%  \n",
      "austen-emma - Chapter 4                     42%  \n",
      "austen-emma - Chapter 5                      0%  \n"
     ]
    }
   ],
   "source": [
    "#B3\n",
    "cosine_sim = np.dot(TFIDF_matrix, TFIDF_matrix.T).toarray()\n",
    "np.fill_diagonal(cosine_sim, 0)\n",
    "max_sim_index = np.unravel_index(np.argmax(cosine_sim), cosine_sim.shape)\n",
    "max_sim_value = cosine_sim[max_sim_index]\n",
    "rounded_value = int((round(max_sim_value, 2)) * 100)\n",
    "print(f\"Most similar pair of documents: {document_labels[max_sim_index[0]]} and {document_labels[max_sim_index[1]]} with a similarity of {rounded_value} percent.\")\n",
    "\n",
    "sim_table = pd.DataFrame(cosine_sim, index=document_labels, columns=document_labels)\n",
    "stringed_sim_table = sim_table.applymap(lambda x: f\"{int(round(x, 2) * 100)}%\")\n",
    "print(\"Similarity table:\")\n",
    "print(stringed_sim_table.iloc[:5, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217417a9",
   "metadata": {},
   "source": [
    "B4 (Reflection)\n",
    "\n",
    "Organizing by chapter was a logical thing to do as it reflects the corpus structure.\n",
    "\n",
    "The top TF-IDF terms reflect the characters/topics that are focused on in the given chapter (which are linked to the actual in-book/corpus chapters). For instance, chapter 1 introduces and focuses most on Emma Woodhouse, Miss Taylor, Mr. Weston, and Mr. Knightly; while chapter 2 looks more into Mr. Weston and Miss Taylor.\n",
    "\n",
    "For the most similar chapters via cosine comparison, chapter 10 directly explores the aftermath after the events of chapter 9, and is also the shortest chapter, which would explain their high similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5a2095",
   "metadata": {},
   "outputs": [],
   "source": [
    "#C1\n",
    "trainer = nltk.tokenize.sent_tokenize\n",
    "sentences = [preprocess(sentence) for sentence in trainer(raw_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f153cf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#C2\n",
    "model = gensim.models.Word2Vec(sentences, vector_size=100, window=5, min_count=3, sg=1, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ebe7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 most similar words to 'emma':\n",
      "  smiling: 0.8791\n",
      "  looked: 0.8436\n",
      "  head: 0.8414\n",
      "  weston: 0.8333\n",
      "  tone: 0.8328\n",
      "  warmly: 0.8300\n",
      "  listened: 0.8291\n",
      "  laughing: 0.8291\n",
      "  trying: 0.8242\n",
      "  voice: 0.8231\n",
      "\n",
      "Top 10 most similar words to 'could':\n",
      "  possible: 0.8472\n",
      "  smallest: 0.8421\n",
      "  less: 0.8376\n",
      "  meaning: 0.8358\n",
      "  reason: 0.8346\n",
      "  sake: 0.8296\n",
      "  spite: 0.8252\n",
      "  degree: 0.8183\n",
      "  answer: 0.8170\n",
      "  difficulty: 0.8155\n",
      "\n",
      "Top 10 most similar words to 'would':\n",
      "  impossible: 0.8969\n",
      "  justice: 0.8883\n",
      "  ashamed: 0.8851\n",
      "  try: 0.8849\n",
      "  must: 0.8838\n",
      "  assured: 0.8759\n",
      "  differently: 0.8746\n",
      "  whenever: 0.8741\n",
      "  reason: 0.8725\n",
      "  judge: 0.8718\n",
      "\n",
      "Top 10 most similar words to 'miss':\n",
      "  poor: 0.7723\n",
      "  niece: 0.7510\n",
      "  told: 0.7194\n",
      "  madam: 0.7180\n",
      "  jane: 0.7117\n",
      "  hoped: 0.7046\n",
      "  obliging: 0.7021\n",
      "  inquiry: 0.6988\n",
      "  oh: 0.6987\n",
      "  thanks: 0.6987\n",
      "\n",
      "Top 10 most similar words to 'must':\n",
      "  chuse: 0.9114\n",
      "  worth: 0.9070\n",
      "  disagreeable: 0.9063\n",
      "  whenever: 0.8995\n",
      "  trouble: 0.8990\n",
      "  agree: 0.8985\n",
      "  dearest: 0.8979\n",
      "  secret: 0.8977\n",
      "  anywhere: 0.8952\n",
      "  missed: 0.8950\n"
     ]
    }
   ],
   "source": [
    "#C3\n",
    "targets = [word for word, _ in freqtoken.most_common(5)]\n",
    "for word in targets:\n",
    "    if word in model.wv:\n",
    "        similar_words = model.wv.most_similar(word, topn=10)\n",
    "        print(f\"\\nTop 10 most similar words to '{word}':\")\n",
    "        for sim_word, score in similar_words:\n",
    "            print(f\"  {sim_word}: {score:.4f}\")\n",
    "    else:\n",
    "        print(f\"\\nWord '{word}' not in vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0c2d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analogy A: [('dancing', 0.8931943774223328)]\n",
      "\n",
      "Analogy B: [('emma', 0.720059335231781)]\n",
      "\n",
      "Analogy C: [('poor', 0.7635173797607422)]\n",
      "\n",
      "Analogy D: [('looking', 0.6551715731620789)]\n",
      "\n",
      "Analogy E: [('smith', 0.8085635900497437)]\n",
      "\n",
      "Analogy F: [('speak', 0.8679779767990112)]\n"
     ]
    }
   ],
   "source": [
    "#C4\n",
    "analogyA = model.wv.most_similar(positive=['mr', 'weston'], negative=['emma'], topn=1)\n",
    "analogyB = model.wv.most_similar(positive=['knightley', 'weston'], negative=['letter'], topn=1)\n",
    "analogyC = model.wv.most_similar(positive=['miss', 'taylor'], negative=['woodhouse'], topn=1)\n",
    "print(f\"\\nAnalogy A: {analogyA}\")\n",
    "print(f\"\\nAnalogy B: {analogyB}\")\n",
    "print(f\"\\nAnalogy C: {analogyC}\")\n",
    "#More to further test performance:\n",
    "analogyD = model.wv.most_similar(positive=['churchill', 'emma'], negative=['never'], topn=1)\n",
    "analogyE = model.wv.most_similar(positive=['taylor', 'woodhouse'], negative=['letter'], topn=1)\n",
    "analogyF = model.wv.most_similar(positive=['elton', 'think'], negative=['miss'], topn=1)\n",
    "print(f\"\\nAnalogy D: {analogyD}\")\n",
    "print(f\"\\nAnalogy E: {analogyE}\")\n",
    "print(f\"\\nAnalogy F: {analogyF}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f99181",
   "metadata": {},
   "source": [
    "C5\n",
    "\n",
    "Concerning neighbours, three of them are remeniscent of stop words and have logical neighbours; the model is correctly confident, given the high accuracy. miss has slightly lower numbers reflecting the fact the model may be getting confused by double meaning of the word (to not hit and the feminine title). There are a few sensable neighbours present here, such as madam and niece. This confusion is 'acknowledged' by the model with slightly lower scores. Emma is the worst offender; as a character with which no standard word is similar to, the model is left to its own devices linking her to other words, such as smiling and added (these may reflect Emma's actions or persona, but this is extremely unlikely and impractical to verify).\n",
    "\n",
    "Analogies within this corpus seem solid, even as three extra were done out of skeptism. All have strong confidence values, and make sense to a certain extent based on vague researching of the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c6c469",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
