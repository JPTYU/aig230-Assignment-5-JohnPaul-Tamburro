{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a635a0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1d5c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2190e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords') \n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6143260",
   "metadata": {},
   "source": [
    "Use this NLTK corpus:\n",
    "Option A (recommended): Gutenberg - 'austen-emma.txt' or 'carroll-alice.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5014db32",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = nltk.corpus.gutenberg.raw('austen-emma.txt')\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccf34d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A1\n",
    "raw_text = nltk.corpus.gutenberg.raw('austen-emma.txt')\n",
    "sentences = nltk.corpus.gutenberg.sents('austen-emma.txt')\n",
    "num_characters = len(raw_text)\n",
    "num_sentences = len(sentences)\n",
    "print(f\"Number of characters: {num_characters}\")\n",
    "print(f\"Number of sentences: {num_sentences}\")\n",
    "print(f\"Total number of tokens BEFORE preprocessing: {len(nltk.word_tokenize(raw_text))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ec3611",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A2\n",
    "def preprocess(text):\n",
    "    # accept either raw string or a token list (e.g., a single sentence)\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "    else:\n",
    "        # treat as an iterable of tokens (e.g., list or sentence from corpus)\n",
    "        tokens = [str(token).lower() for token in text]\n",
    "    stop = nltk.corpus.stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stop]\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "vocabulary = preprocess(raw_text)\n",
    "# preprocess each sentence separately to produce a list of token lists\n",
    "sent_vocab = [preprocess(sent) for sent in sentences]\n",
    "vocabulary_size = len(set(vocabulary))\n",
    "sent_vocabulary_size = len(set([token for sentence in sent_vocab for token in sentence]))\n",
    "freqtoken = nltk.FreqDist(vocabulary)\n",
    "print(f\"Total number of characters (raw text): {len(raw_text)}\")\n",
    "print(f\"Total number of tokens AFTER preprocessing: {len(vocabulary)}\")\n",
    "#This not changing makes sense, as we are working in a character based model.\n",
    "print(f\"Total number of tokens in sentences AFTER preprocessing: {len(sent_vocab)}\")\n",
    "print(f\"Vocabulary size (unique tokens): {vocabulary_size}\")\n",
    "print(f\"Top 20 most frequent tokens: {freqtoken.most_common(20)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9935a89",
   "metadata": {},
   "source": [
    "A3\n",
    "\n",
    "My preprocessing choices will influence various tasks done in other parts. The mandatory preprocessing has already reduced the number of unqiue tokens present, reducing dimensionality and thus preventing overfitting. The initial plan was to keep stop words due to the story-like nature of the corpus, as these stop words contribute to the flow and gramatical refinement of the sentences that make up the story. However, initial running proved this to logic to be impractical and thus stop-words were removed. Using lemmatization instead of stemming will make processes slower, although the end results will be more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85136250",
   "metadata": {},
   "outputs": [],
   "source": [
    "#B1\n",
    "chapters = nltk.corpus.gutenberg.fileids()\n",
    "documents = []\n",
    "document_labels = []\n",
    "\n",
    "for chapter_file_id in chapters:\n",
    "    book_raw_text = nltk.corpus.gutenberg.raw(chapter_file_id)\n",
    "    chapter_strings = book_raw_text.split('CHAPTER')[1:]\n",
    "\n",
    "\n",
    "    for i, chapter_text in enumerate(chapter_strings):\n",
    "        documents.append(chapter_text)\n",
    "        document_labels.append(f\"{chapter_file_id.replace('.txt', '')} - Chapter {i+1}\")\n",
    "\n",
    "processed_documents = [' '.join(preprocess(doc)) for doc in documents]\n",
    "\n",
    "print(chapters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6ee201",
   "metadata": {},
   "outputs": [],
   "source": [
    "#B2\n",
    "print(chapters)\n",
    "BagofWords = CountVectorizer()\n",
    "TFIDF = TfidfVectorizer()\n",
    "BagofWords_matrix = BagofWords.fit_transform(processed_documents)\n",
    "TFIDF_matrix = TFIDF.fit_transform(processed_documents)\n",
    "print(f\"Shape of Bag-of-Words matrix: {BagofWords_matrix.shape}\")\n",
    "print(f\"Shape of TF-IDF matrix: {TFIDF_matrix.shape}\")\n",
    "\n",
    "blake15 = TFIDF_matrix[0].toarray()[0].argsort()[-15:][::-1]\n",
    "blake15_string = TFIDF.get_feature_names_out()[blake15]\n",
    "bryant15 = TFIDF_matrix[1].toarray()[0].argsort()[-15:][::-1]\n",
    "bryant15_string = TFIDF.get_feature_names_out()[bryant15]\n",
    "print(f\"Top 15 TF-IDF terms for first document:\",blake15_string)\n",
    "print(f\"Top 15 TF-IDF terms for second document:\",bryant15_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1553c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#B3\n",
    "cosine_sim = np.dot(TFIDF_matrix, TFIDF_matrix.T).toarray()\n",
    "np.fill_diagonal(cosine_sim, 0)\n",
    "max_sim_index = np.unravel_index(np.argmax(cosine_sim), cosine_sim.shape)\n",
    "max_sim_value = cosine_sim[max_sim_index]\n",
    "rounded_value = int((round(max_sim_value, 2)) * 100)\n",
    "print(f\"Most similar pair of documents: {document_labels[max_sim_index[0]]} and {document_labels[max_sim_index[1]]} with a similarity of {rounded_value} percent.\")\n",
    "\n",
    "sim_table = pd.DataFrame(cosine_sim, index=document_labels, columns=document_labels)\n",
    "stringed_sim_table = sim_table.applymap(lambda x: f\"{int(round(x, 2) * 100)}%\")\n",
    "print(\"Similarity table:\")\n",
    "print(stringed_sim_table.iloc[:5, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217417a9",
   "metadata": {},
   "source": [
    "B4 (Reflection)\n",
    "\n",
    "Organizing by chapter was a logical thing to do as it reflects the corpus structure.\n",
    "\n",
    "The top TF-IDF terms reflect the characters/topics that are focused on in the given chapter (which are linked to the actual in-book/corpus chapters). For instance, chapter 1 introduces and focuses most on Emma Woodhouse, Miss Taylor, Mr. Weston, and Mr. Knightly; while chapter 2 looks more into Mr. Weston and Miss Taylor.\n",
    "\n",
    "For the most similar chapters via cosine comparison, chapter 10 directly explores the aftermath after the events of chapter 9, and is also the shortest chapter, which would explain their high similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5a2095",
   "metadata": {},
   "outputs": [],
   "source": [
    "#C1\n",
    "trainer = nltk.tokenize.sent_tokenize\n",
    "sentences = [preprocess(sentence) for sentence in trainer(raw_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f153cf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#C2\n",
    "model = gensim.models.Word2Vec(sentences, vector_size=100, window=5, min_count=3, sg=1, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ebe7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#C3\n",
    "targets = [word for word, _ in freqtoken.most_common(5)]\n",
    "for word in targets:\n",
    "    if word in model.wv:\n",
    "        similar_words = model.wv.most_similar(word, topn=10)\n",
    "        print(f\"\\nTop 10 most similar words to '{word}':\")\n",
    "        for sim_word, score in similar_words:\n",
    "            print(f\"  {sim_word}: {score:.4f}\")\n",
    "    else:\n",
    "        print(f\"\\nWord '{word}' not in vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0c2d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#C4\n",
    "analogyA = model.wv.most_similar(positive=['mr', 'weston'], negative=['emma'], topn=1)\n",
    "analogyB = model.wv.most_similar(positive=['knightley', 'weston'], negative=['letter'], topn=1)\n",
    "analogyC = model.wv.most_similar(positive=['miss', 'taylor'], negative=['woodhouse'], topn=1)\n",
    "print(f\"\\nAnalogy A: {analogyA}\")\n",
    "print(f\"\\nAnalogy B: {analogyB}\")\n",
    "print(f\"\\nAnalogy C: {analogyC}\")\n",
    "#More to further test performance:\n",
    "analogyD = model.wv.most_similar(positive=['churchill', 'emma'], negative=['never'], topn=1)\n",
    "analogyE = model.wv.most_similar(positive=['taylor', 'woodhouse'], negative=['letter'], topn=1)\n",
    "analogyF = model.wv.most_similar(positive=['elton', 'think'], negative=['miss'], topn=1)\n",
    "print(f\"\\nAnalogy D: {analogyD}\")\n",
    "print(f\"\\nAnalogy E: {analogyE}\")\n",
    "print(f\"\\nAnalogy F: {analogyF}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f99181",
   "metadata": {},
   "source": [
    "C5\n",
    "\n",
    "Concerning neighbours, three of them are remeniscent of stop words and have logical neighbours; the model is correctly confident, given the high accuracy. miss has slightly lower numbers reflecting the fact the model may be getting confused by double meaning of the word (to not hit and the feminine title). There are a few sensable neighbours present here, such as madam and niece. This confusion is 'acknowledged' by the model with slightly lower scores. Emma is the worst offender; as a character with which no standard word is similar to, the model is left to its own devices linking her to other words, such as smiling and added (these may reflect Emma's actions or persona, but this is extremely unlikely and impractical to verify).\n",
    "\n",
    "Analogies within this corpus seem solid, even as three extra were done out of skeptism. All have strong confidence values, and make sense to a certain extent based on vague researching of the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c6c469",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
